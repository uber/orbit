{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a5b7d5",
   "metadata": {},
   "source": [
    "# Model Selection with WBIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95d5a002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0dev\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import orbit\n",
    "print(orbit.__version__)\n",
    "from orbit.models import DLT,ETS, KTRLite, LGT\n",
    "from orbit.utils.simulation import make_trend, make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "organic-somerset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0dev'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orbit.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adjacent-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff376c50",
   "metadata": {},
   "source": [
    "Generate a regression problem with trend with `8` number of regressors where only `3` of them are effective. First, generate the `3` effective regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af05537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (100, 3)\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_REGRESSORS = 8\n",
    "NUM_OF_EFFECTIVE_REGRESSORS = 3\n",
    "SERIES_LEN = 100\n",
    "SEED = 20210101\n",
    "# sample some coefficients\n",
    "COEFS = np.random.default_rng(SEED).uniform(-1, 1, NUM_OF_EFFECTIVE_REGRESSORS)\n",
    "trend = make_trend(SERIES_LEN, rw_loc=0.01, rw_scale=0.1)\n",
    "x, regression, coefs = make_regression(series_len=SERIES_LEN, coefs=COEFS)\n",
    "print(regression.shape, x.shape)\n",
    "\n",
    "# combine trend and the regression\n",
    "y = trend + regression\n",
    "y = y - y.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248509c",
   "metadata": {},
   "source": [
    "We can add `5` irrelevant regressors into the dataset to add challenge in selecting the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac863f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 8)\n"
     ]
    }
   ],
   "source": [
    "x_extra = np.random.normal(0, 1, (SERIES_LEN, NUM_OF_REGRESSORS - NUM_OF_EFFECTIVE_REGRESSORS))\n",
    "x = np.concatenate([x, x_extra], axis=-1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba0c5bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "          y        x1        x2        x3        x4        x5        x6  \\\n",
      "0  3.010526  0.172792  0.000000  0.165219 -1.768846  0.075552 -1.130630   \n",
      "1  3.448303 -0.000000  0.452678  0.223187 -1.274101 -0.061154  0.064514   \n",
      "2  3.012242 -0.000000  0.290559  0.182286 -0.801334  1.312035  1.274699   \n",
      "3  1.422250  0.147066  0.014211  0.273356 -1.444821 -0.368961 -0.769227   \n",
      "4  3.275717 -0.368227 -0.081455 -0.241060  2.089979  0.041971 -0.048341   \n",
      "\n",
      "         x7        x8       date  \n",
      "0 -0.651430 -0.893116 2016-01-10  \n",
      "1  0.410113 -0.572882 2016-01-17  \n",
      "2 -1.214358  0.313719 2016-01-24  \n",
      "3  0.392616  0.057294 2016-01-31  \n",
      "4 -0.513154 -0.084589 2016-02-07  \n"
     ]
    }
   ],
   "source": [
    "x_cols = [f\"x{x}\" for x in range(1, NUM_OF_REGRESSORS + 1)]\n",
    "response_col = \"y\"\n",
    "dt_col = \"date\"\n",
    "obs_matrix = np.concatenate([y.reshape(-1, 1), x], axis=1)\n",
    "# make a data frame for orbit inputs\n",
    "df = pd.DataFrame(obs_matrix, columns=[response_col] + x_cols)\n",
    "# make some dummy date stamp\n",
    "dt = pd.date_range(start='2016-01-04', periods=SERIES_LEN, freq=\"1W\")\n",
    "df['date'] = dt\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76443870",
   "metadata": {},
   "outputs": [],
   "source": [
    " regressor_col = x_cols[:3 + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "split-warning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient evaluation took 0.000347 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.47 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "\n",
      "Gradient evaluation took 0.000309 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.09 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "\n",
      "Gradient evaluation took 0.000239 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 2.39 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "\n",
      "Gradient evaluation took 0.000272 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 2.72 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 3.92767 seconds (Warm-up)\n",
      "               0.444453 seconds (Sampling)\n",
      "               4.37212 seconds (Total)\n",
      "\n",
      "Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 3.94843 seconds (Warm-up)\n",
      "               0.430467 seconds (Sampling)\n",
      "               4.3789 seconds (Total)\n",
      "\n",
      "Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 3.93127 seconds (Warm-up)\n",
      "               0.44741 seconds (Sampling)\n",
      "               4.37868 seconds (Total)\n",
      "\n",
      "Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 4.07598 seconds (Warm-up)\n",
      "               0.483141 seconds (Sampling)\n",
      "               4.55912 seconds (Total)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:n_eff / iter below 0.001 indicates that the effective sample size has likely been overestimated\n",
      "WARNING:pystan:Rhat above 1.1 or below 0.9 indicates that the chains very likely have not mixed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-165.8260620655938"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlt_mod = DLT(\n",
    "        response_col=response_col,\n",
    "        date_col=dt_col,\n",
    "        regressor_col=regressor_col,\n",
    "        seed=2020,\n",
    "        # fixing the smoothing parameters to learn regression coefficients more effectively\n",
    "        level_sm_input=0.01,\n",
    "        slope_sm_input=0.01,\n",
    "        num_warmup=4000,\n",
    "        num_sample=4000,\n",
    "\n",
    "    )\n",
    "dlt_mod.fit_WBIC(df=df) #, , sampling_temperature = np.log(100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "threatened-thanks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient evaluation took 9.5e-05 seconds\n",
      "\n",
      "Gradient evaluation took 0.000108 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 1.08 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "1000 transitions using 10 leapfrog steps per transition would take 0.95 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "\n",
      "Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Gradient evaluation took 9.6e-05 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 0.96 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "\n",
      "Gradient evaluation took 9.6e-05 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 0.96 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 0.07624 seconds (Warm-up)\n",
      "               0.066828 seconds (Sampling)\n",
      "               0.143068 seconds (Total)\n",
      "\n",
      "Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 0.076899 seconds (Warm-up)\n",
      "               0.069591 seconds (Sampling)\n",
      "               0.14649 seconds (Total)\n",
      "\n",
      "Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 0.080365 seconds (Warm-up)\n",
      "               0.072954 seconds (Sampling)\n",
      "               0.153319 seconds (Total)\n",
      "\n",
      "Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 0.077787 seconds (Warm-up)\n",
      "               0.077764 seconds (Sampling)\n",
      "               0.155551 seconds (Total)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:n_eff / iter below 0.001 indicates that the effective sample size has likely been overestimated\n",
      "WARNING:pystan:Rhat above 1.1 or below 0.9 indicates that the chains very likely have not mixed\n",
      "WARNING:pystan:3 of 4000 iterations ended with a divergence (0.075 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-142.83880824930614"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ets_mod = ETS(\n",
    "        response_col=response_col,\n",
    "        date_col=dt_col,\n",
    "        seed=2020,\n",
    "        # fixing the smoothing parameters to learn regression coefficients more effectively\n",
    "        level_sm_input=0.01,\n",
    "        num_warmup=4000,\n",
    "        num_sample=4000,\n",
    "\n",
    "    )\n",
    "\n",
    "ets_mod.fit_WBIC(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "empirical-composer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial log joint probability = -669.995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<orbit.forecaster.map.MAPForecaster at 0x1464b86d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -138.858     0.0607337       37.8001      0.1174       0.801       26   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -137.425   0.000767978       30.2634   3.311e-05       0.001      121  LS failed, Hessian reset \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -137.407   0.000241534       33.9523       4.224      0.4224      149   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -137.405   7.02049e-07       31.6083       0.264      0.8429      180   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -137.405   3.69224e-08       34.0696      0.9116      0.9116      210   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     104      -137.405   5.40302e-07       26.2259   1.684e-08       0.001      275  LS failed, Hessian reset \n",
      "     113      -137.405   5.22551e-09       25.9266      0.1604      0.1604      291   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: absolute parameter change was below tolerance\n"
     ]
    }
   ],
   "source": [
    "KTRLite_mod = KTRLite(\n",
    "        response_col=response_col,\n",
    "        date_col=dt_col,\n",
    "        seed=2020,\n",
    "        # fixing the smoothing parameters to learn regression coefficients more effectively\n",
    "        #level_sm_input=0.01,\n",
    "        estimator='stan-map',\n",
    "        #num_warmup=4000,\n",
    "        #num_sample=4000,\n",
    "\n",
    "    )\n",
    "\n",
    "KTRLite_mod.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "regulated-committee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient evaluation took 0.000259 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 2.59 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "\n",
      "Gradient evaluation took 0.000276 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 2.76 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "\n",
      "Gradient evaluation took 0.000228 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 2.28 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "\n",
      "Gradient evaluation took 0.000244 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 2.44 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 0.591272 seconds (Warm-up)\n",
      "               0.476826 seconds (Sampling)\n",
      "               1.0681 seconds (Total)\n",
      "\n",
      "Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 0.594745 seconds (Warm-up)\n",
      "               0.492522 seconds (Sampling)\n",
      "               1.08727 seconds (Total)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:n_eff / iter below 0.001 indicates that the effective sample size has likely been overestimated\n",
      "WARNING:pystan:Rhat above 1.1 or below 0.9 indicates that the chains very likely have not mixed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 0.599946 seconds (Warm-up)\n",
      "               0.53226 seconds (Sampling)\n",
      "               1.13221 seconds (Total)\n",
      "\n",
      "Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 0.606032 seconds (Warm-up)\n",
      "               0.596136 seconds (Sampling)\n",
      "               1.20217 seconds (Total)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-144.82307440011647"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGT_mod = LGT(\n",
    "        response_col=response_col,\n",
    "        date_col=dt_col,\n",
    "        seed=2020,\n",
    "        # fixing the smoothing parameters to learn regression coefficients more effectively\n",
    "        #level_sm_input=0.01,\n",
    "        estimator='stan-mcmc',\n",
    "        num_warmup=4000,\n",
    "        num_sample=4000,\n",
    "\n",
    "    )\n",
    "\n",
    "LGT_mod.fit_WBIC(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "ets_mod.fit_WBIC(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "ets_mod = ETS(\n",
    "        response_col=response_col,\n",
    "        date_col=dt_col,\n",
    "        seed=2020,\n",
    "        # fixing the smoothing parameters to learn regression coefficients more effectively\n",
    "        level_sm_input=0.01,\n",
    "        num_warmup=4000,\n",
    "        num_sample=4000,\n",
    "\n",
    "    )\n",
    "ets_mod.fit(df=df,sampling_temperature = np.log(100.0)) #, , sampling_temperature = np.log(100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "ets_mod.get_WBIC_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "ForecasterException: Model class: <class 'orbit.template.dlt.DLTModel'> is incompatible with \n",
    "        Estimator: <class 'orbit.estimators.stan_estimator.StanEstimatorMCMC'>.  \n",
    "            Estimator Support: [<class 'orbit.estimators.stan_estimator.StanEstimatorMAP'>, \n",
    "                                <class 'orbit.estimators.stan_estimator.StanEstimatorMCMC'>\n",
    "                                <class 'orbit.estimators.stan_estimator.StanEstimatorMCMC'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2dbc2",
   "metadata": {},
   "source": [
    "Now, we can calculate WBIC and compare them across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wbics = np.empty(NUM_OF_REGRESSORS)\n",
    "\n",
    "for idx in range(NUM_OF_REGRESSORS):\n",
    "    regressor_col = x_cols[:idx + 1]\n",
    "\n",
    "    dlt_mod = DLT(\n",
    "        response_col=response_col,\n",
    "        date_col=dt_col,\n",
    "        regressor_col=regressor_col,\n",
    "        seed=2020,\n",
    "        # fixing the smoothing parameters to learn regression coefficients more effectively\n",
    "        level_sm_input=0.01,\n",
    "        slope_sm_input=0.01,\n",
    "        num_warmup=4000,\n",
    "        num_sample=4000,\n",
    "    )\n",
    "    dlt_mod.fit(df=df)\n",
    "    wbic = dlt.get_training_metrics()['WBIC']\n",
    "    print('Regressors:{} WBIC:{:.5f}'.format(regressor_col, wbic))\n",
    "    wbics[idx] = wbic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4f5bd",
   "metadata": {},
   "source": [
    "We plot the chart with WBICs against number of regressors included.  As we can see, WBIC is lowest when regressors overlapped exactly with the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "ax.plot(np.arange(1, NUM_OF_REGRESSORS + 1), wbics, color='dodgerblue', label='WBICs')\n",
    "ax.axvline(x=3, linestyle='--', color='orange', label='truth')\n",
    "ax.set_xlabel('Number of Regressors')\n",
    "ax.set_ylabel('WBIC')\n",
    "ax.set_title('Model Selection with WBIC')\n",
    "fig.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p37",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
